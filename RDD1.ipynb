{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iQ19MWsut9II",
        "outputId": "1b1007c7-6911-409f-fbb0-7a1b70c00581"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1, 4, 9, 16, 25, 36, 49, 64, 81, 100]\n"
          ]
        }
      ],
      "source": [
        "from pyspark import SparkContext\n",
        "sc=SparkContext(\"local\",\"RDDDemo\")\n",
        "rdd1=sc.parallelize([1,2,3,4,5,6,7,8,9,10])\n",
        "rdd2=rdd1.map(lambda x: x ** 2)\n",
        "print(rdd2.collect())"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark import SparkContext\n",
        "sc=SparkContext.getOrCreate()\n",
        "sentences=[\"Data is booming\",\n",
        "           \"Data is powerful\",\n",
        "           \"spark is a powerful tool to work with data\",\n",
        "           \"spark application runs in cluster\"\n",
        "          ]\n",
        "\n",
        "rdd=sc.parallelize(sentences)\n",
        "\n",
        "stopwords = {'is', 'the', 'a', 'an', 'and', 'or', 'in', 'of', 'on', 'to'}\n",
        "\n",
        "word_count= (\n",
        "              rdd.flatMap(lambda sentence: sentence.lower().split())\n",
        "              .filter(lambda word:word not in stopwords)\n",
        "              .map(lambda word: (word,1))\n",
        "              .reduceByKey(lambda a,b:a+b)\n",
        ")\n",
        "\n",
        "output=word_count.collect()\n",
        "\n",
        "for word,count in output:\n",
        "    print(f\"{word}:{count}\")\n",
        "\n",
        "sc.stop()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7YwnZn-huK3-",
        "outputId": "2c7db8b2-7daf-4be5-962d-1eb2033e9ee1"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "data:3\n",
            "booming:1\n",
            "powerful:2\n",
            "spark:2\n",
            "tool:1\n",
            "work:1\n",
            "with:1\n",
            "application:1\n",
            "runs:1\n",
            "cluster:1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark import SparkContext\n",
        "sc=SparkContext.getOrCreate()\n",
        "\n",
        "numbers=[5,3,4,5,2,3,5,3,4]\n",
        "\n",
        "rdd=sc.parallelize(numbers)\n",
        "\n",
        "freq_numbers=(\n",
        "              rdd.map(lambda x:(x,1))\n",
        "              .reduceByKey(lambda a,b:a+b)\n",
        "              .map(lambda x: (x[1],x[0]))\n",
        "              .sortByKey(ascending=False)\n",
        "              .map(lambda x: (x[1],x[0]))\n",
        "              .take(3)\n",
        ")\n",
        "\n",
        "\n",
        "for number,freq in freq_numbers:\n",
        "    print(f\"{number}:{freq}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R2AOts2puOzr",
        "outputId": "80b028af-544b-4ce5-8b15-0ce3c469583b"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5:3\n",
            "3:3\n",
            "4:2\n"
          ]
        }
      ]
    }
  ]
}